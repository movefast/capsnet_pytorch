{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 16\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MNIST Dataset \n",
    "train_dataset = dsets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    transform=transforms.ToTensor(),  \n",
    "    download=True,\n",
    ")\n",
    "\n",
    "test_dataset = dsets.MNIST(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAACDCAYAAACp4J7uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46dee0a3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 5\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    sample_image = transforms.Compose([transforms.ToPILImage(), transforms.Scale(28)])(train_dataset[index][0])\n",
    "    plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 5\n",
       " 0\n",
       " 4\n",
       " 1\n",
       " 9\n",
       "[torch.LongTensor of size 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.train_labels[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Capsules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caps1_n_maps = 32\n",
    "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
    "caps1_n_dims = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_size = caps1_n_maps * caps1_n_dims\n",
    "layer = nn.Sequential(\n",
    "    nn.Conv2d(1, feature_size, kernel_size=9),\n",
    "    nn.BatchNorm2d(feature_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(feature_size, feature_size, kernel_size=9, stride=2),\n",
    "    nn.BatchNorm2d(feature_size),\n",
    "    nn.ReLU(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=1, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When iteration starts, queue and thread start to load dataset from files.\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,0 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "   0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.2510  0.7490\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  1.0000  1.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.2510  1.0000  1.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.7490  1.0000  1.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  1.0000  1.0000  1.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  1.0000  1.0000  1.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  1.0000  1.0000  1.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.2510  1.0000  1.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.5020\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.5020\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.7490\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.5020\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 9 to 17 \n",
       "   0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.2510\n",
       "  0.0000  0.0000  0.2510  0.2510  0.0000  0.0000  0.0000  0.2510  1.0000\n",
       "  0.0000  0.0000  0.7490  1.0000  0.0000  0.0000  0.0000  1.0000  1.0000\n",
       "  0.7490  1.0000  1.0000  0.5020  0.0000  0.0000  0.2510  1.0000  1.0000\n",
       "  1.0000  1.0000  0.2510  0.0000  0.0000  0.2510  1.0000  1.0000  1.0000\n",
       "  1.0000  0.5020  0.0000  0.0000  0.0000  0.7490  1.0000  1.0000  0.7490\n",
       "  1.0000  0.0000  0.0000  0.0000  1.0000  1.0000  1.0000  1.0000  0.2510\n",
       "  0.5020  0.0000  0.0000  0.5020  1.0000  1.0000  1.0000  0.2510  0.0000\n",
       "  0.2510  0.0000  0.2510  1.0000  1.0000  1.0000  0.2510  0.0000  0.0000\n",
       "  0.7490  0.5020  0.7490  1.0000  1.0000  0.5020  0.0000  0.0000  0.0000\n",
       "  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
       "  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
       "  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
       "  1.0000  1.0000  1.0000  0.2510  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  1.0000  1.0000  1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  1.0000  1.0000  0.7490  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  1.0000  1.0000  0.5020  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.5020  0.5020  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 18 to 26 \n",
       "   0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.2510  0.5020  1.0000  0.5020  0.0000  0.0000  0.0000  0.0000\n",
       "  0.2510  1.0000  1.0000  1.0000  0.5020  0.0000  0.0000  0.0000  0.0000\n",
       "  1.0000  1.0000  1.0000  0.7490  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  1.0000  1.0000  0.5020  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  1.0000  1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.5020  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.5020  0.5020  0.0000  0.0000\n",
       "  0.5020  0.5020  0.5020  0.5020  1.0000  1.0000  1.0000  0.5020  0.0000\n",
       "  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  0.0000\n",
       "  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  0.7490  0.2510  0.0000\n",
       "  1.0000  0.7490  0.5020  0.5020  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 27 to 27 \n",
       "   0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "[torch.FloatTensor of size 1x1x28x28]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_image = Variable(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = layer(example_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0  ,.,.) = \n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.1686  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0853  0.3195\n",
       "  0.0000  0.7998  0.0000  ...   0.0000  0.1497  0.0000\n",
       "           ...             ⋱             ...          \n",
       "  0.0978  0.0000  0.0206  ...   0.0000  0.0000  0.0705\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "[torch.FloatTensor of size 1x1152x8]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(-1, caps1_n_caps, caps1_n_dims)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(tensor, dim=-1, epsilon=1e-7):\n",
    "    squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "    safe_norm = torch.sqrt(squared_norm + epsilon)\n",
    "    scale = squared_norm / (1 + squared_norm)\n",
    "    return scale * tensor / safe_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0  ,.,.) = \n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0360  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0254  0.0953\n",
       "  0.0000  0.3790  0.0000  ...   0.0000  0.0709  0.0000\n",
       "           ...             ⋱             ...          \n",
       "  0.0408  0.0000  0.0086  ...   0.0000  0.0000  0.0295\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "[torch.FloatTensor of size 1x1152x8]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = squash(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Capsules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Predicted Output Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caps2_n_caps = 10\n",
    "caps2_n_dims = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1152, 10, 16, 8])\n"
     ]
    }
   ],
   "source": [
    "init_sigma = 0.01\n",
    "W_init = torch.Tensor(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims)\n",
    "W_init = Variable(W_init.normal_(std=init_sigma))\n",
    "print(W_init.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1152, 10, 8, 1])\n"
     ]
    }
   ],
   "source": [
    "x = x[:,:,None,:,None].repeat(1,1,10,1,1)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1152, 10, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "caps2_predicted = W_init @ x\n",
    "print(caps2_predicted.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing by agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1152, 10, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "raw_weights = Variable(torch.zeros(1, caps1_n_caps, caps2_n_caps, 1, 1))\n",
    "print(raw_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1152, 10, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "routing_weights = F.softmax(raw_weights,dim=2)\n",
    "print(routing_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1152, 10, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "weighted_predictions = routing_weights * caps2_predicted\n",
    "print(weighted_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 10, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "weighted_sum = weighted_predictions.sum(dim=1, keepdim=True)\n",
    "print(weighted_sum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caps2_output_round_1 = squash(weighted_sum, dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1152, 10, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "caps2_output_round_1_tiled = caps2_output_round_1.repeat(1, caps1_n_caps, 1, 1, 1)\n",
    "print(caps2_output_round_1_tiled.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1152, 10, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "agreement = caps2_predicted.transpose(-1, -2) @ caps2_output_round_1_tiled\n",
    "print(agreement.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1152, 10, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "raw_weights_round_2 = raw_weights + agreement\n",
    "print(raw_weights_round_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1152, 10, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "routing_weights_round_2 = F.softmax(raw_weights_round_2,dim=2)\n",
    "print(routing_weights_round_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1152, 10, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "weighted_predictions_round_2 = routing_weights_round_2 * caps2_predicted\n",
    "print(routing_weights_round_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 10, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "weighted_sum_round_2 = weighted_predictions_round_2.sum(dim=1, keepdim=True)\n",
    "print(weighted_sum_round_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caps2_output_round_2 = squash(weighted_sum_round_2, dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caps2_output = caps2_output_round_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimated Class Probabilities (Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_norm(tensor, dim=-1, epsilon=1e-7, keepdim=False):\n",
    "    squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "    return torch.sqrt(squared_norm + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 10, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "y_proba = safe_norm(caps2_output, dim=-2)\n",
    "print(y_proba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba_argmax = torch.max(y_proba, dim=2)[1]\n",
    "y_proba_argmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = torch.squeeze(y_proba_argmax)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 4\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = labels.float()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margin loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lambda_ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = Variable(torch.eye(num_classes).index_select(dim=0, index=labels))\n",
    "# T = torch.Tensor(batch_size, num_classes).scatter_(1, torch.LongTensor(labels)[:,None],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,0 ,0 ,.,.) = \n",
       "1.00000e-03 *\n",
       "  -0.2325\n",
       "  -0.3222\n",
       "   2.9180\n",
       "  -0.1554\n",
       "   0.9319\n",
       "  -0.6061\n",
       "  -2.4007\n",
       "   0.8849\n",
       "  -0.5356\n",
       "   0.5044\n",
       "  -2.3594\n",
       "   3.0347\n",
       "   2.1489\n",
       "  -0.4929\n",
       "   1.9136\n",
       "   0.6000\n",
       "\n",
       "(0 ,0 ,1 ,.,.) = \n",
       "1.00000e-03 *\n",
       "  -1.2087\n",
       "   1.8070\n",
       "   0.8415\n",
       "  -0.3420\n",
       "  -0.4328\n",
       "  -1.6493\n",
       "   2.3560\n",
       "   0.9145\n",
       "  -0.3877\n",
       "   0.9408\n",
       "   0.4515\n",
       "  -1.0223\n",
       "  -1.0542\n",
       "   0.8221\n",
       "  -0.4048\n",
       "  -0.7833\n",
       "\n",
       "(0 ,0 ,2 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   1.0503\n",
       "   1.3430\n",
       "   0.5619\n",
       "  -1.2391\n",
       "   0.8155\n",
       "  -0.5785\n",
       "  -0.6234\n",
       "   0.6751\n",
       "  -0.5354\n",
       "  -0.8307\n",
       "   0.4379\n",
       "   0.2040\n",
       "  -0.4175\n",
       "  -0.1044\n",
       "  -0.1206\n",
       "  -0.7733\n",
       "\n",
       "(0 ,0 ,3 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   1.2199\n",
       "  -0.7606\n",
       "   0.8090\n",
       "  -0.1018\n",
       "  -0.9332\n",
       "   0.4343\n",
       "   0.9381\n",
       "   0.5927\n",
       "  -0.4354\n",
       "   0.8690\n",
       "   0.1962\n",
       "   0.7631\n",
       "  -0.4178\n",
       "   1.1347\n",
       "  -0.6458\n",
       "  -2.1344\n",
       "\n",
       "(0 ,0 ,4 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   1.7153\n",
       "   1.2189\n",
       "   0.1188\n",
       "  -0.0064\n",
       "   1.5830\n",
       "  -0.0556\n",
       "  -0.8988\n",
       "  -0.4481\n",
       "   0.1761\n",
       "  -0.8921\n",
       "   0.2861\n",
       "   1.2341\n",
       "   0.3663\n",
       "  -0.0960\n",
       "   0.1012\n",
       "   0.0134\n",
       "\n",
       "(0 ,0 ,5 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   1.1028\n",
       "  -0.1683\n",
       "  -0.2558\n",
       "   0.4930\n",
       "   0.0839\n",
       "   1.0521\n",
       "  -0.2528\n",
       "   0.3408\n",
       "  -0.6340\n",
       "  -0.3185\n",
       "   0.0609\n",
       "  -1.0868\n",
       "   0.7869\n",
       "  -0.2020\n",
       "   0.3607\n",
       "  -0.1085\n",
       "\n",
       "(0 ,0 ,6 ,.,.) = \n",
       "1.00000e-03 *\n",
       "  -0.9085\n",
       "   1.4053\n",
       "   1.6581\n",
       "   0.7580\n",
       "   0.4446\n",
       "   2.6634\n",
       "   1.6496\n",
       "   0.8845\n",
       "   0.6686\n",
       "   0.5713\n",
       "   0.4919\n",
       "  -0.4420\n",
       "   3.2192\n",
       "  -0.4178\n",
       "   0.0310\n",
       "   0.7007\n",
       "\n",
       "(0 ,0 ,7 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   2.6674\n",
       "  -1.4964\n",
       "   3.4523\n",
       "  -1.6240\n",
       "  -0.4877\n",
       "   1.8780\n",
       "   2.2015\n",
       "   1.4018\n",
       "  -0.8825\n",
       "  -1.2835\n",
       "   0.7362\n",
       "  -1.4173\n",
       "   2.6146\n",
       "  -0.8756\n",
       "  -1.5676\n",
       "   5.0105\n",
       "\n",
       "(0 ,0 ,8 ,.,.) = \n",
       "1.00000e-03 *\n",
       "  -0.3784\n",
       "  -3.5465\n",
       "   1.7903\n",
       "  -0.3417\n",
       "  -1.4393\n",
       "  -0.1108\n",
       "  -1.9371\n",
       "   1.2114\n",
       "   1.4068\n",
       "  -0.4046\n",
       "  -2.0201\n",
       "   0.2461\n",
       "   0.9743\n",
       "  -1.8156\n",
       "   1.6314\n",
       "   0.7479\n",
       "\n",
       "(0 ,0 ,9 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   0.7837\n",
       "  -1.3934\n",
       "  -1.2871\n",
       "  -0.2153\n",
       "   0.4007\n",
       "  -0.7081\n",
       "   0.5920\n",
       "  -0.3616\n",
       "   0.0732\n",
       "   0.9178\n",
       "  -0.5516\n",
       "  -0.8659\n",
       "  -1.0159\n",
       "  -0.0704\n",
       "   0.3286\n",
       "   1.0754\n",
       "[torch.FloatTensor of size 1x1x10x16x1]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caps2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 10, 1, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caps2_output_norm = safe_norm(caps2_output, dim=-2, keepdim=True)\n",
    "caps2_output_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.7985  0.8020  0.8047  0.8035  0.8041  0.8058  0.8003  0.7945  0.7991  0.8044\n",
       "[torch.FloatTensor of size 1x10]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#         reconstruction_loss = self.reconstruction_loss(reconstructions, images)\n",
    "\n",
    "#         return (margin_loss + 0.0005 * reconstruction_loss) / images.size(0)\n",
    "present_error_raw = F.relu(m_plus - caps2_output_norm, inplace=True) ** 2\n",
    "present_error = present_error_raw.view(-1, 10)\n",
    "present_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    0     0     0     0     0     0     0     0     0     0\n",
       "[torch.FloatTensor of size 1x10]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absent_error_raw = F.relu(caps2_output_norm - m_minus, inplace=True) ** 2\n",
    "absent_error = absent_error_raw.view(-1, 10)\n",
    "absent_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0000  0.0000  0.0000  0.0000  0.8041  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "[torch.FloatTensor of size 1x10]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = T * present_error + lambda_ * (1. - T) * absent_error\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.8041\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "margin_loss = torch.mean(torch.sum(L))\n",
    "margin_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,0 ,0 ,.,.) = \n",
       "1.00000e-03 *\n",
       "  -0.2325\n",
       "  -0.3222\n",
       "   2.9180\n",
       "  -0.1554\n",
       "   0.9319\n",
       "  -0.6061\n",
       "  -2.4007\n",
       "   0.8849\n",
       "  -0.5356\n",
       "   0.5044\n",
       "  -2.3594\n",
       "   3.0347\n",
       "   2.1489\n",
       "  -0.4929\n",
       "   1.9136\n",
       "   0.6000\n",
       "\n",
       "(0 ,0 ,1 ,.,.) = \n",
       "1.00000e-03 *\n",
       "  -1.2087\n",
       "   1.8070\n",
       "   0.8415\n",
       "  -0.3420\n",
       "  -0.4328\n",
       "  -1.6493\n",
       "   2.3560\n",
       "   0.9145\n",
       "  -0.3877\n",
       "   0.9408\n",
       "   0.4515\n",
       "  -1.0223\n",
       "  -1.0542\n",
       "   0.8221\n",
       "  -0.4048\n",
       "  -0.7833\n",
       "\n",
       "(0 ,0 ,2 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   1.0503\n",
       "   1.3430\n",
       "   0.5619\n",
       "  -1.2391\n",
       "   0.8155\n",
       "  -0.5785\n",
       "  -0.6234\n",
       "   0.6751\n",
       "  -0.5354\n",
       "  -0.8307\n",
       "   0.4379\n",
       "   0.2040\n",
       "  -0.4175\n",
       "  -0.1044\n",
       "  -0.1206\n",
       "  -0.7733\n",
       "\n",
       "(0 ,0 ,3 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   1.2199\n",
       "  -0.7606\n",
       "   0.8090\n",
       "  -0.1018\n",
       "  -0.9332\n",
       "   0.4343\n",
       "   0.9381\n",
       "   0.5927\n",
       "  -0.4354\n",
       "   0.8690\n",
       "   0.1962\n",
       "   0.7631\n",
       "  -0.4178\n",
       "   1.1347\n",
       "  -0.6458\n",
       "  -2.1344\n",
       "\n",
       "(0 ,0 ,4 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   1.7153\n",
       "   1.2189\n",
       "   0.1188\n",
       "  -0.0064\n",
       "   1.5830\n",
       "  -0.0556\n",
       "  -0.8988\n",
       "  -0.4481\n",
       "   0.1761\n",
       "  -0.8921\n",
       "   0.2861\n",
       "   1.2341\n",
       "   0.3663\n",
       "  -0.0960\n",
       "   0.1012\n",
       "   0.0134\n",
       "\n",
       "(0 ,0 ,5 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   1.1028\n",
       "  -0.1683\n",
       "  -0.2558\n",
       "   0.4930\n",
       "   0.0839\n",
       "   1.0521\n",
       "  -0.2528\n",
       "   0.3408\n",
       "  -0.6340\n",
       "  -0.3185\n",
       "   0.0609\n",
       "  -1.0868\n",
       "   0.7869\n",
       "  -0.2020\n",
       "   0.3607\n",
       "  -0.1085\n",
       "\n",
       "(0 ,0 ,6 ,.,.) = \n",
       "1.00000e-03 *\n",
       "  -0.9085\n",
       "   1.4053\n",
       "   1.6581\n",
       "   0.7580\n",
       "   0.4446\n",
       "   2.6634\n",
       "   1.6496\n",
       "   0.8845\n",
       "   0.6686\n",
       "   0.5713\n",
       "   0.4919\n",
       "  -0.4420\n",
       "   3.2192\n",
       "  -0.4178\n",
       "   0.0310\n",
       "   0.7007\n",
       "\n",
       "(0 ,0 ,7 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   2.6674\n",
       "  -1.4964\n",
       "   3.4523\n",
       "  -1.6240\n",
       "  -0.4877\n",
       "   1.8780\n",
       "   2.2015\n",
       "   1.4018\n",
       "  -0.8825\n",
       "  -1.2835\n",
       "   0.7362\n",
       "  -1.4173\n",
       "   2.6146\n",
       "  -0.8756\n",
       "  -1.5676\n",
       "   5.0105\n",
       "\n",
       "(0 ,0 ,8 ,.,.) = \n",
       "1.00000e-03 *\n",
       "  -0.3784\n",
       "  -3.5465\n",
       "   1.7903\n",
       "  -0.3417\n",
       "  -1.4393\n",
       "  -0.1108\n",
       "  -1.9371\n",
       "   1.2114\n",
       "   1.4068\n",
       "  -0.4046\n",
       "  -2.0201\n",
       "   0.2461\n",
       "   0.9743\n",
       "  -1.8156\n",
       "   1.6314\n",
       "   0.7479\n",
       "\n",
       "(0 ,0 ,9 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   0.7837\n",
       "  -1.3934\n",
       "  -1.2871\n",
       "  -0.2153\n",
       "   0.4007\n",
       "  -0.7081\n",
       "   0.5920\n",
       "  -0.3616\n",
       "   0.0732\n",
       "   0.9178\n",
       "  -0.5516\n",
       "  -0.8659\n",
       "  -1.0159\n",
       "  -0.0704\n",
       "   0.3286\n",
       "   1.0754\n",
       "[torch.FloatTensor of size 1x1x10x16x1]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caps2_output #.index_select(dim=2, index=Variable(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    0     0     0     0     0     0     0     1     0     0\n",
       "[torch.FloatTensor of size 1x10]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_one_hot = Variable(torch.eye(num_classes).index_select(dim=0, index=y_pred.data))\n",
    "y_pred_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask_with_labels = False # default to use predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,0 ,0 ,.,.) = \n",
       "  0\n",
       "\n",
       "(0 ,0 ,1 ,.,.) = \n",
       "  0\n",
       "\n",
       "(0 ,0 ,2 ,.,.) = \n",
       "  0\n",
       "\n",
       "(0 ,0 ,3 ,.,.) = \n",
       "  0\n",
       "\n",
       "(0 ,0 ,4 ,.,.) = \n",
       "  0\n",
       "\n",
       "(0 ,0 ,5 ,.,.) = \n",
       "  0\n",
       "\n",
       "(0 ,0 ,6 ,.,.) = \n",
       "  0\n",
       "\n",
       "(0 ,0 ,7 ,.,.) = \n",
       "  1\n",
       "\n",
       "(0 ,0 ,8 ,.,.) = \n",
       "  0\n",
       "\n",
       "(0 ,0 ,9 ,.,.) = \n",
       "  0\n",
       "[torch.FloatTensor of size 1x1x10x1x1]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_mask = T if mask_with_labels else y_pred_one_hot\n",
    "# T.view(-1, 1, caps2_n_caps, 1, 1) * caps2_output\n",
    "reconstruction_mask_reshaped = reconstruction_mask.view(-1, 1, caps2_n_caps, 1, 1)\n",
    "reconstruction_mask_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,0 ,0 ,.,.) = \n",
       "1.00000e-03 *\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "\n",
       "(0 ,0 ,1 ,.,.) = \n",
       "1.00000e-03 *\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "\n",
       "(0 ,0 ,2 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "\n",
       "(0 ,0 ,3 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "\n",
       "(0 ,0 ,4 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "\n",
       "(0 ,0 ,5 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "\n",
       "(0 ,0 ,6 ,.,.) = \n",
       "1.00000e-03 *\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "\n",
       "(0 ,0 ,7 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   2.6674\n",
       "  -1.4964\n",
       "   3.4523\n",
       "  -1.6240\n",
       "  -0.4877\n",
       "   1.8780\n",
       "   2.2015\n",
       "   1.4018\n",
       "  -0.8825\n",
       "  -1.2835\n",
       "   0.7362\n",
       "  -1.4173\n",
       "   2.6146\n",
       "  -0.8756\n",
       "  -1.5676\n",
       "   5.0105\n",
       "\n",
       "(0 ,0 ,8 ,.,.) = \n",
       "1.00000e-03 *\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "\n",
       "(0 ,0 ,9 ,.,.) = \n",
       "1.00000e-03 *\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "  -0.0000\n",
       "   0.0000\n",
       "   0.0000\n",
       "[torch.FloatTensor of size 1x1x10x16x1]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caps2_output_masked = reconstruction_mask_reshaped * caps2_output\n",
    "caps2_output_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       "1.00000e-03 *\n",
       " -0.0000 -0.0000  0.0000 -0.0000  0.0000 -0.0000 -0.0000  0.0000 -0.0000  0.0000\n",
       "\n",
       "Columns 10 to 19 \n",
       "1.00000e-03 *\n",
       " -0.0000  0.0000  0.0000 -0.0000  0.0000  0.0000 -0.0000  0.0000  0.0000 -0.0000\n",
       "\n",
       "Columns 20 to 29 \n",
       "1.00000e-03 *\n",
       " -0.0000 -0.0000  0.0000  0.0000 -0.0000  0.0000  0.0000 -0.0000 -0.0000  0.0000\n",
       "\n",
       "Columns 30 to 39 \n",
       "1.00000e-03 *\n",
       " -0.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000  0.0000 -0.0000 -0.0000  0.0000\n",
       "\n",
       "Columns 40 to 49 \n",
       "1.00000e-03 *\n",
       " -0.0000 -0.0000  0.0000  0.0000 -0.0000 -0.0000 -0.0000 -0.0000  0.0000 -0.0000\n",
       "\n",
       "Columns 50 to 59 \n",
       "1.00000e-03 *\n",
       "  0.0000 -0.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 60 to 69 \n",
       "1.00000e-03 *\n",
       " -0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000  0.0000 -0.0000\n",
       "\n",
       "Columns 70 to 79 \n",
       "1.00000e-03 *\n",
       " -0.0000 -0.0000  0.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 80 to 89 \n",
       "1.00000e-03 *\n",
       "  0.0000 -0.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000  0.0000 -0.0000 -0.0000\n",
       "\n",
       "Columns 90 to 99 \n",
       "1.00000e-03 *\n",
       "  0.0000 -0.0000  0.0000 -0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 100 to 109 \n",
       "1.00000e-03 *\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000 -0.0000  0.0000 -0.0000\n",
       "\n",
       "Columns 110 to 119 \n",
       "1.00000e-03 *\n",
       "  0.0000  0.0000  2.6674 -1.4964  3.4523 -1.6240 -0.4877  1.8780  2.2015  1.4018\n",
       "\n",
       "Columns 120 to 129 \n",
       "1.00000e-03 *\n",
       " -0.8825 -1.2835  0.7362 -1.4173  2.6146 -0.8756 -1.5676  5.0105 -0.0000 -0.0000\n",
       "\n",
       "Columns 130 to 139 \n",
       "1.00000e-03 *\n",
       "  0.0000 -0.0000 -0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000 -0.0000  0.0000\n",
       "\n",
       "Columns 140 to 149 \n",
       "1.00000e-03 *\n",
       "  0.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000 -0.0000  0.0000 -0.0000\n",
       "\n",
       "Columns 150 to 159 \n",
       "1.00000e-03 *\n",
       "  0.0000 -0.0000  0.0000  0.0000 -0.0000 -0.0000 -0.0000 -0.0000  0.0000  0.0000\n",
       "[torch.FloatTensor of size 1x160]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input = caps2_output_masked.view(-1, caps2_n_caps * caps2_n_dims)\n",
    "decoder_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden1 = 512\n",
    "n_hidden2 = 1024\n",
    "n_output = 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.4882  0.5021  0.4890  0.5001  0.4970  0.5050  0.5035  0.5010  0.4928  0.5022\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.5020  0.4920  0.5079  0.4925  0.4970  0.4975  0.4926  0.4933  0.5067  0.5028\n",
       "\n",
       "Columns 20 to 29 \n",
       " 0.4977  0.4976  0.5062  0.4972  0.5056  0.4944  0.5055  0.5046  0.4973  0.4979\n",
       "\n",
       "Columns 30 to 39 \n",
       " 0.5101  0.5045  0.4958  0.4956  0.4949  0.5034  0.5019  0.5022  0.4927  0.4938\n",
       "\n",
       "Columns 40 to 49 \n",
       " 0.4999  0.5026  0.5050  0.5027  0.5017  0.4962  0.5102  0.5055  0.5013  0.5049\n",
       "\n",
       "Columns 50 to 59 \n",
       " 0.5051  0.5044  0.4978  0.4992  0.4956  0.5029  0.4933  0.4964  0.5022  0.4954\n",
       "\n",
       "Columns 60 to 69 \n",
       " 0.5039  0.4962  0.5075  0.5060  0.4983  0.5030  0.4974  0.4896  0.5034  0.5104\n",
       "\n",
       "Columns 70 to 79 \n",
       " 0.4937  0.4886  0.4894  0.5010  0.5126  0.5010  0.4903  0.5020  0.5044  0.5048\n",
       "\n",
       "Columns 80 to 89 \n",
       " 0.4966  0.4987  0.4932  0.5082  0.4926  0.5087  0.4953  0.4985  0.5015  0.4972\n",
       "\n",
       "Columns 90 to 99 \n",
       " 0.4962  0.4933  0.4992  0.4942  0.5057  0.4897  0.5013  0.4967  0.4954  0.5017\n",
       "\n",
       "Columns 100 to 109 \n",
       " 0.4989  0.5045  0.5096  0.5081  0.4878  0.4903  0.4961  0.5071  0.4919  0.4977\n",
       "\n",
       "Columns 110 to 119 \n",
       " 0.5006  0.4913  0.4992  0.5063  0.5071  0.4998  0.4910  0.4954  0.5031  0.5062\n",
       "\n",
       "Columns 120 to 129 \n",
       " 0.5020  0.5100  0.5040  0.4996  0.5086  0.4905  0.4974  0.5031  0.5079  0.4998\n",
       "\n",
       "Columns 130 to 139 \n",
       " 0.4999  0.5058  0.4986  0.4991  0.5001  0.5012  0.4952  0.5011  0.4972  0.5074\n",
       "\n",
       "Columns 140 to 149 \n",
       " 0.5050  0.5033  0.5025  0.5076  0.4984  0.5008  0.5051  0.4991  0.5098  0.4984\n",
       "\n",
       "Columns 150 to 159 \n",
       " 0.5027  0.4945  0.4974  0.4985  0.4978  0.4969  0.5031  0.4958  0.5005  0.5119\n",
       "\n",
       "Columns 160 to 169 \n",
       " 0.4997  0.5022  0.4979  0.5079  0.4858  0.5109  0.4974  0.4999  0.4995  0.5043\n",
       "\n",
       "Columns 170 to 179 \n",
       " 0.5019  0.5061  0.5056  0.5034  0.4930  0.5031  0.5048  0.4960  0.5007  0.5090\n",
       "\n",
       "Columns 180 to 189 \n",
       " 0.5069  0.5018  0.4905  0.4944  0.4959  0.4931  0.4952  0.4958  0.4932  0.4927\n",
       "\n",
       "Columns 190 to 199 \n",
       " 0.5035  0.4999  0.5019  0.5003  0.5102  0.4941  0.4987  0.4976  0.4987  0.5066\n",
       "\n",
       "Columns 200 to 209 \n",
       " 0.4930  0.4910  0.4963  0.5108  0.4999  0.5033  0.4973  0.5050  0.4933  0.4971\n",
       "\n",
       "Columns 210 to 219 \n",
       " 0.5024  0.5024  0.5001  0.5076  0.4921  0.4998  0.5038  0.5031  0.5048  0.4982\n",
       "\n",
       "Columns 220 to 229 \n",
       " 0.5038  0.5065  0.4974  0.4933  0.4952  0.4905  0.5045  0.4902  0.5002  0.5071\n",
       "\n",
       "Columns 230 to 239 \n",
       " 0.4987  0.5044  0.5032  0.5018  0.4979  0.4944  0.4867  0.4922  0.4991  0.4969\n",
       "\n",
       "Columns 240 to 249 \n",
       " 0.4997  0.4965  0.5026  0.5030  0.4969  0.5058  0.5025  0.5007  0.5039  0.5078\n",
       "\n",
       "Columns 250 to 259 \n",
       " 0.5043  0.4912  0.4975  0.5008  0.5013  0.4902  0.4911  0.5076  0.5054  0.4952\n",
       "\n",
       "Columns 260 to 269 \n",
       " 0.5014  0.4927  0.5029  0.5108  0.4869  0.5015  0.4990  0.4988  0.5017  0.5046\n",
       "\n",
       "Columns 270 to 279 \n",
       " 0.5011  0.4985  0.4894  0.5062  0.4938  0.5020  0.5037  0.5039  0.4889  0.4998\n",
       "\n",
       "Columns 280 to 289 \n",
       " 0.5062  0.4970  0.4993  0.5063  0.4998  0.5049  0.5047  0.4975  0.4971  0.4983\n",
       "\n",
       "Columns 290 to 299 \n",
       " 0.4985  0.5015  0.5048  0.4967  0.4908  0.5002  0.4957  0.5011  0.5005  0.4961\n",
       "\n",
       "Columns 300 to 309 \n",
       " 0.5030  0.5029  0.4912  0.5027  0.4959  0.4995  0.5010  0.4969  0.4989  0.4954\n",
       "\n",
       "Columns 310 to 319 \n",
       " 0.4986  0.4950  0.5007  0.5073  0.5059  0.5070  0.5091  0.5048  0.4950  0.5092\n",
       "\n",
       "Columns 320 to 329 \n",
       " 0.5051  0.4982  0.5055  0.5066  0.4942  0.4996  0.4975  0.5036  0.5034  0.4976\n",
       "\n",
       "Columns 330 to 339 \n",
       " 0.4970  0.5018  0.5001  0.5066  0.4974  0.5011  0.4988  0.4956  0.5012  0.5027\n",
       "\n",
       "Columns 340 to 349 \n",
       " 0.4938  0.4999  0.4902  0.5076  0.5062  0.4967  0.4967  0.4991  0.4937  0.5020\n",
       "\n",
       "Columns 350 to 359 \n",
       " 0.4944  0.4980  0.4875  0.5007  0.4990  0.5052  0.4925  0.5037  0.5059  0.5041\n",
       "\n",
       "Columns 360 to 369 \n",
       " 0.4934  0.4958  0.4991  0.5079  0.5069  0.4963  0.5035  0.5043  0.4970  0.4932\n",
       "\n",
       "Columns 370 to 379 \n",
       " 0.5019  0.4914  0.5027  0.5003  0.4960  0.4940  0.5014  0.5002  0.5104  0.5089\n",
       "\n",
       "Columns 380 to 389 \n",
       " 0.4910  0.4944  0.4986  0.5102  0.4952  0.5053  0.5029  0.4888  0.4922  0.5032\n",
       "\n",
       "Columns 390 to 399 \n",
       " 0.4973  0.5009  0.4992  0.5003  0.4983  0.4958  0.5038  0.4941  0.5041  0.4976\n",
       "\n",
       "Columns 400 to 409 \n",
       " 0.4947  0.4938  0.4939  0.5043  0.5068  0.5033  0.5087  0.4891  0.4942  0.5048\n",
       "\n",
       "Columns 410 to 419 \n",
       " 0.4944  0.5025  0.4996  0.5033  0.4971  0.4940  0.5139  0.5082  0.5042  0.4965\n",
       "\n",
       "Columns 420 to 429 \n",
       " 0.5043  0.4983  0.5005  0.4977  0.4915  0.5007  0.5045  0.4980  0.4904  0.4921\n",
       "\n",
       "Columns 430 to 439 \n",
       " 0.4912  0.5032  0.5006  0.4969  0.5075  0.5012  0.5048  0.5028  0.4972  0.5030\n",
       "\n",
       "Columns 440 to 449 \n",
       " 0.5016  0.5004  0.4866  0.5129  0.5048  0.5018  0.5015  0.4978  0.5042  0.5034\n",
       "\n",
       "Columns 450 to 459 \n",
       " 0.5013  0.4878  0.4911  0.5022  0.4961  0.5011  0.5088  0.4954  0.4961  0.4991\n",
       "\n",
       "Columns 460 to 469 \n",
       " 0.5086  0.4956  0.5055  0.5040  0.4988  0.5005  0.4939  0.5032  0.4894  0.5045\n",
       "\n",
       "Columns 470 to 479 \n",
       " 0.5015  0.4964  0.5005  0.4939  0.5046  0.5042  0.4959  0.5058  0.5000  0.4962\n",
       "\n",
       "Columns 480 to 489 \n",
       " 0.5102  0.4933  0.4974  0.4915  0.4983  0.5007  0.5018  0.4940  0.5034  0.5001\n",
       "\n",
       "Columns 490 to 499 \n",
       " 0.5037  0.4907  0.5008  0.5038  0.5035  0.5086  0.5050  0.5028  0.4897  0.5010\n",
       "\n",
       "Columns 500 to 509 \n",
       " 0.4962  0.5000  0.4930  0.5053  0.5044  0.4964  0.4987  0.5003  0.5045  0.4918\n",
       "\n",
       "Columns 510 to 519 \n",
       " 0.5046  0.4966  0.5043  0.4892  0.4950  0.5040  0.5018  0.5022  0.4994  0.5074\n",
       "\n",
       "Columns 520 to 529 \n",
       " 0.4925  0.4992  0.4945  0.5040  0.5025  0.4955  0.5095  0.5030  0.4932  0.4984\n",
       "\n",
       "Columns 530 to 539 \n",
       " 0.5065  0.4976  0.5090  0.4913  0.4973  0.4956  0.5082  0.5002  0.5051  0.4932\n",
       "\n",
       "Columns 540 to 549 \n",
       " 0.4936  0.5092  0.4913  0.5024  0.5075  0.5034  0.4997  0.4959  0.5070  0.4972\n",
       "\n",
       "Columns 550 to 559 \n",
       " 0.4996  0.5063  0.4999  0.5022  0.4984  0.4991  0.5058  0.5076  0.4945  0.4989\n",
       "\n",
       "Columns 560 to 569 \n",
       " 0.4989  0.4996  0.5065  0.4927  0.5059  0.5031  0.5037  0.5036  0.4978  0.4976\n",
       "\n",
       "Columns 570 to 579 \n",
       " 0.4973  0.5025  0.5125  0.4951  0.4982  0.4976  0.5076  0.4960  0.5000  0.5088\n",
       "\n",
       "Columns 580 to 589 \n",
       " 0.4966  0.4908  0.4936  0.5007  0.5064  0.5088  0.4958  0.4951  0.5073  0.4970\n",
       "\n",
       "Columns 590 to 599 \n",
       " 0.5033  0.4998  0.5091  0.5033  0.5010  0.5094  0.5020  0.4916  0.5050  0.5070\n",
       "\n",
       "Columns 600 to 609 \n",
       " 0.4906  0.4923  0.5059  0.4925  0.4957  0.5018  0.5004  0.4909  0.4968  0.4936\n",
       "\n",
       "Columns 610 to 619 \n",
       " 0.4879  0.4918  0.5081  0.4980  0.4992  0.5011  0.5023  0.4975  0.4993  0.4926\n",
       "\n",
       "Columns 620 to 629 \n",
       " 0.5022  0.4902  0.5070  0.5040  0.5013  0.4895  0.4977  0.5016  0.4996  0.5042\n",
       "\n",
       "Columns 630 to 639 \n",
       " 0.4964  0.4961  0.4979  0.4987  0.4955  0.5119  0.5008  0.4947  0.5008  0.4882\n",
       "\n",
       "Columns 640 to 649 \n",
       " 0.5015  0.5102  0.4976  0.5096  0.4955  0.4976  0.4948  0.4971  0.4959  0.4991\n",
       "\n",
       "Columns 650 to 659 \n",
       " 0.4920  0.5078  0.5026  0.4942  0.5055  0.5024  0.4909  0.4910  0.4973  0.5018\n",
       "\n",
       "Columns 660 to 669 \n",
       " 0.4997  0.4951  0.5025  0.5066  0.4930  0.4969  0.4967  0.5042  0.5010  0.4929\n",
       "\n",
       "Columns 670 to 679 \n",
       " 0.4974  0.5059  0.5093  0.4924  0.5026  0.5112  0.5037  0.4915  0.4988  0.4973\n",
       "\n",
       "Columns 680 to 689 \n",
       " 0.4918  0.5004  0.5016  0.4961  0.5038  0.5043  0.5008  0.4906  0.4927  0.4927\n",
       "\n",
       "Columns 690 to 699 \n",
       " 0.5034  0.5001  0.4918  0.5005  0.4966  0.5023  0.5057  0.5003  0.4921  0.5034\n",
       "\n",
       "Columns 700 to 709 \n",
       " 0.4946  0.5077  0.5025  0.4886  0.5032  0.4897  0.5026  0.4941  0.4972  0.4988\n",
       "\n",
       "Columns 710 to 719 \n",
       " 0.5074  0.5095  0.5024  0.5026  0.5015  0.4980  0.5034  0.5058  0.5096  0.4975\n",
       "\n",
       "Columns 720 to 729 \n",
       " 0.5063  0.5111  0.5073  0.5022  0.5086  0.4932  0.5025  0.5012  0.5060  0.5029\n",
       "\n",
       "Columns 730 to 739 \n",
       " 0.4973  0.4968  0.5044  0.5062  0.5018  0.4886  0.4956  0.4946  0.5025  0.4970\n",
       "\n",
       "Columns 740 to 749 \n",
       " 0.5068  0.5038  0.5057  0.4899  0.5133  0.5085  0.4878  0.5001  0.4953  0.5080\n",
       "\n",
       "Columns 750 to 759 \n",
       " 0.4991  0.4985  0.4926  0.5100  0.4970  0.5051  0.5041  0.4997  0.4984  0.5072\n",
       "\n",
       "Columns 760 to 769 \n",
       " 0.5037  0.5021  0.4916  0.4937  0.4975  0.4985  0.5124  0.4941  0.4942  0.4918\n",
       "\n",
       "Columns 770 to 779 \n",
       " 0.5004  0.4923  0.5082  0.4951  0.4998  0.4895  0.4974  0.5009  0.4936  0.4915\n",
       "\n",
       "Columns 780 to 783 \n",
       " 0.4994  0.5037  0.4961  0.5005\n",
       "[torch.FloatTensor of size 1x784]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output = nn.Sequential(\n",
    "    nn.Linear(caps2_n_caps * caps2_n_dims, n_hidden1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(n_hidden1, n_hidden2),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(n_hidden2, n_output),\n",
    "    nn.Sigmoid(),\n",
    ")(decoder_input)\n",
    "decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAACQCAAAAADCLb1kAAAJiklEQVR4nN1c2ZbDuAqk7slnz79z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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=144x144 at 0x7F396D271780>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "transforms.Compose([transforms.ToPILImage(), transforms.Scale(144)])(decoder_output.data.view(-1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 183.4392\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_loss = nn.MSELoss(size_average=False)(decoder_output, example_image.view(batch_size,-1))\n",
    "reconstruction_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.8959\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = margin_loss + alpha * reconstruction_loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Touches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = y == y_pred.data.float()\n",
    "torch.mean(correct.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.8959\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer = Adam()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PrimaryCapsuleLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, caps1_n_maps=32, caps1_n_dims=8):\n",
    "        super(PrimaryCapsuleLayer, self).__init__()\n",
    "        self.caps1_n_maps = caps1_n_maps\n",
    "        self.caps1_n_dims = caps1_n_dims\n",
    "        self.caps1_n_caps = caps1_n_maps * 6 * 6 # 1152 primary capsules\n",
    "        \n",
    "        feature_size = caps1_n_maps * caps1_n_dims\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(1, feature_size, kernel_size=9),\n",
    "            nn.BatchNorm2d(feature_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(feature_size, feature_size, kernel_size=9, stride=2),\n",
    "            nn.BatchNorm2d(feature_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def squash(self, tensor, dim=-1, epsilon=1e-7):\n",
    "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "        safe_norm = torch.sqrt(squared_norm + epsilon)\n",
    "        scale = squared_norm / (1 + squared_norm)\n",
    "        return scale * tensor / safe_norm\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer(x)\n",
    "        out = out.view(-1, self.caps1_n_caps, self.caps1_n_dims)\n",
    "        out = self.squash(out)\n",
    "        return out\n",
    "    \n",
    "class DigitCapsuleLayer(nn.Module):\n",
    "    \n",
    "    init_sigma = 0.01\n",
    "    \n",
    "    def __init__(self, caps1_n_caps=1152, caps1_n_dims=8, caps2_n_caps=10, caps2_n_dims=16):\n",
    "        super(DigitCapsuleLayer, self).__init__()\n",
    "        self.caps2_n_caps = caps2_n_caps\n",
    "        self.caps1_n_caps = caps1_n_caps\n",
    "        W_init = torch.Tensor(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims)\n",
    "        W_init.normal_(std=self.init_sigma)\n",
    "        self.w_init = nn.Parameter(W_init).cuda()\n",
    "        \n",
    "    def squash(self, tensor, dim=-1, epsilon=1e-7):\n",
    "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "        safe_norm = torch.sqrt(squared_norm + epsilon)\n",
    "        scale = squared_norm / (1 + squared_norm)\n",
    "        return scale * tensor / safe_norm\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # round 1\n",
    "        x_ = x[:,:,None,:,None].repeat(1,1,10,1,1)\n",
    "        caps2_predicted = self.w_init @ x_\n",
    "        raw_weights = Variable(torch.zeros(1, self.caps1_n_caps, self.caps2_n_caps, 1, 1)).cuda()\n",
    "        routing_weights = F.softmax(raw_weights,dim=2)\n",
    "        weighted_predictions = routing_weights * caps2_predicted\n",
    "        weighted_sum = weighted_predictions.sum(dim=1, keepdim=True)\n",
    "        caps2_output_round_1 = self.squash(weighted_sum, dim=-2)\n",
    "        \n",
    "        # round 2\n",
    "        caps2_output_round_1_tiled = caps2_output_round_1.repeat(1, self.caps1_n_caps, 1, 1, 1)\n",
    "        agreement = caps2_predicted.transpose(-1, -2) @ caps2_output_round_1_tiled\n",
    "        raw_weights_round_2 = raw_weights + agreement\n",
    "        routing_weights_round_2 = F.softmax(raw_weights_round_2,dim=2)\n",
    "        weighted_predictions_round_2 = routing_weights_round_2 * caps2_predicted\n",
    "        weighted_sum_round_2 = weighted_predictions_round_2.sum(dim=1, keepdim=True)\n",
    "        caps2_output_round_2 = self.squash(weighted_sum_round_2, dim=-2)\n",
    "        caps2_output = caps2_output_round_2\n",
    "        return caps2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self, caps2_n_caps=10, caps2_n_dims=16):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.primary_capsules = PrimaryCapsuleLayer()\n",
    "        self.digit_capsules = DigitCapsuleLayer()\n",
    "        self.caps2_n_caps = caps2_n_caps\n",
    "        self.caps2_n_dims = caps2_n_dims\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16 * num_classes, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, labels):\n",
    "        out = self.primary_capsules(x)\n",
    "        caps2_output = self.digit_capsules(out)\n",
    "        \n",
    "        def safe_norm(tensor, dim=-1, epsilon=1e-7, keepdim=False):\n",
    "            squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "            return torch.sqrt(squared_norm + epsilon)\n",
    "\n",
    "        y_proba = safe_norm(caps2_output, dim=-2)\n",
    "        y_proba_argmax = torch.max(y_proba, dim=2)[1]\n",
    "        y_pred = torch.squeeze(y_proba_argmax)\n",
    "        \n",
    "#         m_plus = 0.9\n",
    "#         m_minus = 0.1\n",
    "#         lambda_ = 0.5\n",
    "\n",
    "        T = Variable(torch.eye(num_classes).index_select(dim=0, index=labels)).cuda()\n",
    "#         caps2_output_norm = safe_norm(caps2_output, dim=-2, keepdim=True)\n",
    "\n",
    "#         present_error_raw = F.relu(m_plus - caps2_output_norm, inplace=True) ** 2\n",
    "#         present_error = present_error_raw.view(-1, 10)\n",
    "\n",
    "#         absent_error_raw = F.relu(caps2_output_norm - m_minus, inplace=True) ** 2\n",
    "#         absent_error = absent_error_raw.view(-1, 10)\n",
    "\n",
    "#         L = T * present_error + lambda_ * (1. - T) * absent_error\n",
    "\n",
    "#         margin_loss = torch.mean(torch.sum(L))\n",
    "        \n",
    "        y_pred_one_hot = Variable(torch.eye(num_classes).index_select(dim=0, index=y_pred.cpu().data)).cuda()\n",
    "        mask_with_labels = self.training # default to use predictions\n",
    "\n",
    "        reconstruction_mask = T if mask_with_labels else y_pred_one_hot\n",
    "        reconstruction_mask_reshaped = reconstruction_mask.view(-1, 1, self.caps2_n_caps, 1, 1)\n",
    "\n",
    "        caps2_output_masked = reconstruction_mask_reshaped * caps2_output\n",
    "        decoder_input = caps2_output_masked.view(-1, self.caps2_n_caps * self.caps2_n_dims)\n",
    "\n",
    "        n_hidden1 = 512\n",
    "        n_hidden2 = 1024\n",
    "        n_output = 28 * 28\n",
    "\n",
    "#         decoder_output = nn.Sequential(\n",
    "#             nn.Linear(caps2_n_caps * caps2_n_dims, n_hidden1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(n_hidden1, n_hidden2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(n_hidden2, n_output),\n",
    "#             nn.Sigmoid(),\n",
    "#         )(decoder_input)\n",
    "        decoder_output = self.decoder(decoder_input)\n",
    "\n",
    "#         reconstruction_loss = self.reconstruction_loss(decoder_output, x.view(batch_size,-1))\n",
    "\n",
    "#         alpha = 0.0005\n",
    "\n",
    "#         loss = margin_loss + alpha * reconstruction_loss\n",
    "#         return loss\n",
    "        return caps2_output, y_pred, decoder_output\n",
    "    \n",
    "class CapsuleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsuleLoss, self).__init__()\n",
    "        self.reconstruction_loss = nn.MSELoss(size_average=False)\n",
    "    \n",
    "    def forward(self, x, labels, caps2_output, y_pred, decoder_output):\n",
    "        m_plus = 0.9\n",
    "        m_minus = 0.1\n",
    "        lambda_ = 0.5\n",
    "        \n",
    "        def safe_norm(tensor, dim=-1, epsilon=1e-7, keepdim=False):\n",
    "            squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "            return torch.sqrt(squared_norm + epsilon)\n",
    "\n",
    "        T = Variable(torch.eye(num_classes).index_select(dim=0, index=labels)).cuda()\n",
    "        caps2_output_norm = safe_norm(caps2_output, dim=-2, keepdim=True)\n",
    "\n",
    "        present_error_raw = F.relu(m_plus - caps2_output_norm, inplace=True) ** 2\n",
    "        present_error = present_error_raw.view(-1, 10)\n",
    "\n",
    "        absent_error_raw = F.relu(caps2_output_norm - m_minus, inplace=True) ** 2\n",
    "        absent_error = absent_error_raw.view(-1, 10)\n",
    "\n",
    "        L = T * present_error + lambda_ * (1. - T) * absent_error\n",
    "\n",
    "        margin_loss = torch.mean(torch.sum(L))\n",
    "        \n",
    "        reconstruction_loss = self.reconstruction_loss(decoder_output, x.view(batch_size,-1))\n",
    "\n",
    "        alpha = 0.0005\n",
    "\n",
    "        loss = margin_loss + alpha * reconstruction_loss\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of CapsuleNet(\n",
       "  (primary_capsules): PrimaryCapsuleLayer(\n",
       "    (layer): Sequential(\n",
       "      (0): Conv2d (1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d (256, 256, kernel_size=(9, 9), stride=(2, 2))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (digit_capsules): DigitCapsuleLayer(\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=160, out_features=512)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Linear(in_features=512, out_features=1024)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Linear(in_features=1024, out_features=784)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "model = CapsuleNet()\n",
    "model.cuda()\n",
    "criterion = CapsuleLoss().cuda()\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader)\n",
    "\n",
    "# for _ in range(5):\n",
    "#     images, labels = data_iter.next()\n",
    "#     images = Variable(images).cuda()\n",
    "#     labels = labels\n",
    "\n",
    "#     # Forward + Backward + Optimize\n",
    "#     optimizer.zero_grad()\n",
    "#     caps2_output, y_pred, decoder_output = model(images, labels)\n",
    "#     loss = CapsuleLoss()(images, labels, caps2_output, y_pred, decoder_output)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Iter [3750/3750] Loss: 0.6020\n",
      "Epoch [2/5], Iter [3750/3750] Loss: 0.3940\n",
      "Epoch [3/5], Iter [3750/3750] Loss: 0.2034\n",
      "Epoch [4/5], Iter [3750/3750] Loss: 0.1927\n",
      "Epoch [5/5], Iter [3750/3750] Loss: 0.1587\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images).cuda()\n",
    "        labels = labels\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        caps2_output, y_pred, decoder_output = model(images, labels)\n",
    "        loss = criterion(images, labels, caps2_output, y_pred, decoder_output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = data_iter.next()\n",
    "images = Variable(images).cuda()\n",
    "labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAACPCAYAAADeIl6VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46d5045828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAACPCAYAAADeIl6VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPpUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46d504d3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "caps2_output, y_pred, decoder_output = model(images, labels)\n",
    "reshaped_images = images.cpu().data.numpy().reshape(-1, 28, 28)\n",
    "reconstructions = decoder_output.view(-1,28,28).cpu().data.numpy()\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.imshow(reshaped_images[index], cmap=\"binary\")\n",
    "    plt.title(\"Label:\" + str(labels[index]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.title(\"Predicted:\" + str(int(y_pred[index])))\n",
    "    plt.imshow(reconstructions[index], cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training with Ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ignite.trainer import Trainer, TrainingEvents\n",
    "import logging\n",
    "from ignite.handlers.logging import log_training_simple_moving_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logger = logging.getLogger('ignite')\n",
    "# logger.handlers = []\n",
    "# handler = logging.StreamHandler()\n",
    "# logger.addHandler(handler)\n",
    "# logger.setLevel(logging.INFO)\n",
    "logger = print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_log_validation_loss_and_accuracy_handler(logger):\n",
    "    def log_validation_loss_and_accuracy(trainer):\n",
    "        len_dataset = len(trainer.validation_data.dataset)\n",
    "        num_batches = int(len_dataset / batch_size)\n",
    "        avg_loss = np.mean([loss for (loss, accuracy) in trainer.validation_history[-num_batches:]])\n",
    "        accuracy = sum([accuracy for (loss, accuracy) in trainer.validation_history[-num_batches:]])\n",
    "        logger('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            avg_loss, accuracy, len_dataset,\n",
    "            (accuracy * 100.) / len_dataset,\n",
    "        ))\n",
    "    return log_validation_loss_and_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch[0/5] Iteration[500/3750 (13.33%)]\tCapsule Loss Simple Moving Average: 1.7493\n",
      "Training Epoch[0/5] Iteration[1000/3750 (26.67%)]\tCapsule Loss Simple Moving Average: 1.7007\n",
      "Training Epoch[0/5] Iteration[1500/3750 (40.00%)]\tCapsule Loss Simple Moving Average: 1.7355\n",
      "Training Epoch[0/5] Iteration[2000/3750 (53.33%)]\tCapsule Loss Simple Moving Average: 1.7894\n",
      "Training Epoch[0/5] Iteration[2500/3750 (66.67%)]\tCapsule Loss Simple Moving Average: 1.7963\n",
      "Training Epoch[0/5] Iteration[3000/3750 (80.00%)]\tCapsule Loss Simple Moving Average: 1.7811\n",
      "Training Epoch[0/5] Iteration[3500/3750 (93.33%)]\tCapsule Loss Simple Moving Average: 1.8114\n",
      "\n",
      "Validation set: Average loss: 1.9190, Accuracy: 9793/10000 (98%)\n",
      "\n",
      "Training Epoch[1/5] Iteration[250/3750 (6.67%)]\tCapsule Loss Simple Moving Average: 1.7399\n",
      "Training Epoch[1/5] Iteration[750/3750 (20.00%)]\tCapsule Loss Simple Moving Average: 1.6607\n",
      "Training Epoch[1/5] Iteration[1250/3750 (33.33%)]\tCapsule Loss Simple Moving Average: 1.7328\n",
      "Training Epoch[1/5] Iteration[1750/3750 (46.67%)]\tCapsule Loss Simple Moving Average: 1.6807\n",
      "Training Epoch[1/5] Iteration[2250/3750 (60.00%)]\tCapsule Loss Simple Moving Average: 1.7339\n",
      "Training Epoch[1/5] Iteration[2750/3750 (73.33%)]\tCapsule Loss Simple Moving Average: 1.7140\n",
      "Training Epoch[1/5] Iteration[3250/3750 (86.67%)]\tCapsule Loss Simple Moving Average: 1.8026\n",
      "Training Epoch[1/5] Iteration[0/3750 (0.00%)]\tCapsule Loss Simple Moving Average: 1.7500\n",
      "\n",
      "Validation set: Average loss: 1.9088, Accuracy: 9803/10000 (98%)\n",
      "\n",
      "Training Epoch[2/5] Iteration[500/3750 (13.33%)]\tCapsule Loss Simple Moving Average: 1.6902\n",
      "Training Epoch[2/5] Iteration[1000/3750 (26.67%)]\tCapsule Loss Simple Moving Average: 1.6820\n",
      "Training Epoch[2/5] Iteration[1500/3750 (40.00%)]\tCapsule Loss Simple Moving Average: 1.6433\n",
      "Training Epoch[2/5] Iteration[2000/3750 (53.33%)]\tCapsule Loss Simple Moving Average: 1.7739\n",
      "Training Epoch[2/5] Iteration[2500/3750 (66.67%)]\tCapsule Loss Simple Moving Average: 1.7129\n",
      "Training Epoch[2/5] Iteration[3000/3750 (80.00%)]\tCapsule Loss Simple Moving Average: 1.7102\n",
      "Training Epoch[2/5] Iteration[3500/3750 (93.33%)]\tCapsule Loss Simple Moving Average: 1.6894\n",
      "\n",
      "Validation set: Average loss: 1.9003, Accuracy: 9792/10000 (98%)\n",
      "\n",
      "Training Epoch[3/5] Iteration[250/3750 (6.67%)]\tCapsule Loss Simple Moving Average: 1.6141\n",
      "Training Epoch[3/5] Iteration[750/3750 (20.00%)]\tCapsule Loss Simple Moving Average: 1.6095\n",
      "Training Epoch[3/5] Iteration[1250/3750 (33.33%)]\tCapsule Loss Simple Moving Average: 1.6832\n",
      "Training Epoch[3/5] Iteration[1750/3750 (46.67%)]\tCapsule Loss Simple Moving Average: 1.6798\n",
      "Training Epoch[3/5] Iteration[2250/3750 (60.00%)]\tCapsule Loss Simple Moving Average: 1.6560\n",
      "Training Epoch[3/5] Iteration[2750/3750 (73.33%)]\tCapsule Loss Simple Moving Average: 1.7562\n",
      "Training Epoch[3/5] Iteration[3250/3750 (86.67%)]\tCapsule Loss Simple Moving Average: 1.7492\n",
      "Training Epoch[3/5] Iteration[0/3750 (0.00%)]\tCapsule Loss Simple Moving Average: 1.6421\n",
      "\n",
      "Validation set: Average loss: 1.8797, Accuracy: 9799/10000 (98%)\n",
      "\n",
      "Training Epoch[4/5] Iteration[500/3750 (13.33%)]\tCapsule Loss Simple Moving Average: 1.6225\n",
      "Training Epoch[4/5] Iteration[1000/3750 (26.67%)]\tCapsule Loss Simple Moving Average: 1.6364\n",
      "Training Epoch[4/5] Iteration[1500/3750 (40.00%)]\tCapsule Loss Simple Moving Average: 1.6159\n",
      "Training Epoch[4/5] Iteration[2000/3750 (53.33%)]\tCapsule Loss Simple Moving Average: 1.6433\n",
      "Training Epoch[4/5] Iteration[2500/3750 (66.67%)]\tCapsule Loss Simple Moving Average: 1.6326\n",
      "Training Epoch[4/5] Iteration[3000/3750 (80.00%)]\tCapsule Loss Simple Moving Average: 1.5851\n",
      "Training Epoch[4/5] Iteration[3500/3750 (93.33%)]\tCapsule Loss Simple Moving Average: 1.6700\n",
      "\n",
      "Validation set: Average loss: 1.8793, Accuracy: 9797/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_training_function(is_training=False):\n",
    "    def training_update_function(batch):\n",
    "        if is_training:\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        optimizer.zero_grad()\n",
    "        images, labels = Variable(batch[0]).cuda(), batch[1]\n",
    "        caps2_output, y_pred, decoder_output = model(images, labels)\n",
    "        loss = criterion(images, labels, caps2_output, y_pred, decoder_output)\n",
    "        if is_training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return loss.data[0]\n",
    "        else:\n",
    "            correct = y_pred.data.cpu().eq(labels.view_as(y_pred)).sum()\n",
    "            return loss.data[0], correct\n",
    "    return training_update_function\n",
    "\n",
    "# def training_update_function(batch):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     images, labels = Variable(batch[0]).cuda(), batch[1]\n",
    "#     caps2_output, y_pred, decoder_output = model(images, labels)\n",
    "#     loss = criterion(images, labels, caps2_output, y_pred, decoder_output)\n",
    "#     correct = y_pred.data.cpu().eq(labels.view_as(y_pred)).sum()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return loss.data[0], correct\n",
    "\n",
    "# def val_update_function(batch):\n",
    "#     model.eval()\n",
    "#     optimizer.zero_grad()\n",
    "#     images, labels = Variable(batch[0]).cuda(), batch[1]\n",
    "#     caps2_output, y_pred, decoder_output = model(images, labels)\n",
    "#     loss = criterion(images, labels, caps2_output, y_pred, decoder_output)\n",
    "#     correct = y_pred.data.cpu().eq(labels.view_as(y_pred)).sum()\n",
    "#     return loss.data[0], correct\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    train_loader, \n",
    "    get_training_function(True), \n",
    "    validation_data=test_loader, \n",
    "    validation_inference_function=get_training_function(),\n",
    ")\n",
    "trainer.add_event_handler(TrainingEvents.VALIDATION_COMPLETED, get_log_validation_loss_and_accuracy_handler(logger))\n",
    "trainer.add_event_handler(\n",
    "    TrainingEvents.TRAINING_ITERATION_COMPLETED,\n",
    "    log_training_simple_moving_average,\n",
    "    window_size=100,\n",
    "    metric_name=\"Capsule Loss\",\n",
    "    should_log=lambda trainer: trainer.current_iteration % 500 == 0,\n",
    "    logger=logger,\n",
    ")\n",
    "trainer.run(max_epochs=5, validate_every_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images, labels = iter(test_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images = Variable(images).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caps2_output, y_pred, decoder_output = model(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.data.cpu().eq(labels.view_as(y_pred)).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
